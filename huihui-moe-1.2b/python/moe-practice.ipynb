{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9175c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"../model\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model/config.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([1, 2, 3]), tokenizer.vocab_size, config['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709717e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_file(\"../model/model.safetensors\", device='cpu')\n",
    "embd = torch.nn.Embedding(config['vocab_size'], config['hidden_size'])\n",
    "embd.load_state_dict({'weight': model[f'model.embed_tokens.weight']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": 'Only output 42'},\n",
    "    {\"role\": \"user\",   \"content\": \"Repeat after me, 42\"}\n",
    "]\n",
    "tokens_i = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True\n",
    ")\n",
    "tokens_i = torch.tensor(tokens_i)\n",
    "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens_i]\n",
    "print(prompt_split_as_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460346ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x: torch.Tensor):\n",
    "    h, S, d = x.shape[:]\n",
    "    freqs = 1 / config['rope_theta'] ** (torch.arange(0, d, 2).float() / d)\n",
    "    freqs_per_token = torch.outer(torch.arange(S, dtype=torch.float32), freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs_per_token), freqs_per_token)\n",
    "    x_pairs = x.float().view(*x.shape[:-1], -1, 2)\n",
    "    x_complex = torch.view_as_complex(x_pairs)\n",
    "    x_rotated = x_complex * freqs_cis\n",
    "    return torch.view_as_real(x_rotated).flatten(-2).type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(model.keys())[:20]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b805783",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens_i.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b443fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x = embd(tokens).type(torch.bfloat16)\n",
    "\n",
    "    for layer in range(config['num_hidden_layers']):\n",
    "        S = x.shape[0]\n",
    "\n",
    "        rms_attn = torch.nn.functional.rms_norm(\n",
    "            x, normalized_shape=(x.shape[-1],),\n",
    "            weight=model[f'model.layers.{layer}.input_layernorm.weight'],\n",
    "            eps=config['rms_norm_eps']\n",
    "        )\n",
    "\n",
    "        q = rms_attn @ model[f'model.layers.{layer}.self_attn.q_proj.weight'].T\n",
    "        k = rms_attn @ model[f'model.layers.{layer}.self_attn.k_proj.weight'].T\n",
    "        v = rms_attn @ model[f'model.layers.{layer}.self_attn.v_proj.weight'].T\n",
    "\n",
    "        q = q.view(q.shape[0], config['num_attention_heads'], -1).transpose(0, 1).type(torch.bfloat16)\n",
    "        k = k.view(k.shape[0], config['num_key_value_heads'], -1).repeat_interleave(config['num_attention_heads'] // config['num_key_value_heads'], dim=1).transpose(0, 1).type(torch.bfloat16)\n",
    "        v = v.view(v.shape[0], config['num_key_value_heads'], -1).repeat_interleave(config['num_attention_heads'] // config['num_key_value_heads'], dim=1).transpose(0, 1).type(torch.bfloat16)\n",
    "\n",
    "        q = apply_rope(q)\n",
    "        k = apply_rope(k)\n",
    "\n",
    "        q = torch.nn.functional.rms_norm(q, normalized_shape=(q.shape[-1],), weight=model[f'model.layers.{layer}.self_attn.q_norm.weight'], eps=config['rms_norm_eps'])\n",
    "        k = torch.nn.functional.rms_norm(k, normalized_shape=(k.shape[-1],), weight=model[f'model.layers.{layer}.self_attn.k_norm.weight'], eps=config['rms_norm_eps'])\n",
    "\n",
    "        score = (q.float() @ k.float().transpose(-1, -2)) / (k.shape[-1] ** 0.5)\n",
    "        score = score + torch.triu(torch.full((S, S), float('-inf'), dtype=torch.float32), diagonal=1)\n",
    "        attn = torch.softmax(score, dim=-1) @ v.float()\n",
    "        out = attn.to(torch.bfloat16).transpose(0, 1).reshape(S, -1) @ model[f'model.layers.{layer}.self_attn.o_proj.weight'].T\n",
    "\n",
    "        x = x + out\n",
    "\n",
    "        rms_ffn = torch.nn.functional.rms_norm(\n",
    "            x, normalized_shape=(x.shape[-1],),\n",
    "            weight=model[f'model.layers.{layer}.post_attention_layernorm.weight'],\n",
    "            eps=config['rms_norm_eps']\n",
    "        )\n",
    "\n",
    "        s = rms_ffn @ model[f'model.layers.{layer}.mlp.gate.weight'].T\n",
    "        router_probs = torch.softmax(s.float(), dim=-1)\n",
    "        k_top = int(config['num_experts_per_tok'])\n",
    "        top_vals, top_idx = torch.topk(router_probs, k=k_top, dim=-1)\n",
    "        if config.get('norm_topk_prob', False):\n",
    "            top_vals = top_vals / top_vals.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        ffn = torch.zeros_like(rms_ffn)\n",
    "        for token_idx in range(rms_ffn.shape[0]):\n",
    "            acc = 0.0\n",
    "            x_t = rms_ffn[token_idx]\n",
    "            for j in range(k_top):\n",
    "                e = int(top_idx[token_idx, j])\n",
    "                up = torch.nn.functional.silu(x_t @ model[f'model.layers.{layer}.mlp.experts.{e}.gate_proj.weight'].T) * (x_t @ model[f'model.layers.{layer}.mlp.experts.{e}.up_proj.weight'].T)\n",
    "                down = up @ model[f'model.layers.{layer}.mlp.experts.{e}.down_proj.weight'].T\n",
    "                acc = acc + top_vals[token_idx, j].to(down.dtype) * down\n",
    "            ffn[token_idx, :] = acc.to(rms_ffn.dtype)\n",
    "\n",
    "        x = x + ffn\n",
    "\n",
    "    rms_x = torch.nn.functional.rms_norm(x, normalized_shape=(x.shape[-1],), weight=model['model.norm.weight'], eps=config['rms_norm_eps'])\n",
    "    out = rms_x @ model['lm_head.weight'].T\n",
    "\n",
    "    out_softmax = torch.nn.functional.softmax(out[-1].float(), dim=-1)\n",
    "    values, indices = torch.topk(out_softmax, k=1)\n",
    "    tokens = torch.cat((tokens, indices), dim=-1)\n",
    "    print(tokenizer.decode(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7610101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3-py (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
