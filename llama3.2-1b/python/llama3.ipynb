{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2236c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9175c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"../model/tokenizer.model\"\n",
    "special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n",
    "mergeable_ranks = load_tiktoken_bpe(tokenizer_path)\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name,\n",
    "    pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b7462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model/params.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709717e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../model/consolidated.00.pth\", map_location=torch.device('cpu'))\n",
    "embd = torch.nn.Embedding(tokenizer.n_vocab, config['dim'])\n",
    "embd.load_state_dict({'weight': model['tok_embeddings.weight']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "tokens_i = [128000] + tokenizer.encode(prompt)\n",
    "tokens_i = torch.tensor(tokens_i)\n",
    "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens_i]\n",
    "print(prompt_split_as_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((tokens_i, torch.tensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b96832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement ROPE\n",
    "def apply_rope(x, start_pos=0):\n",
    "    \"\"\"Apply rotary positional encoding to queries or keys\"\"\"\n",
    "    seq_len, head_dim = x.shape[-2], x.shape[-1]\n",
    "    \n",
    "    # Create base frequencies\n",
    "    freqs = 1.0 / (config['rope_theta'] ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "    \n",
    "    # Create frequency for each position\n",
    "    freqs_for_each_token = torch.outer(torch.arange(start_pos, start_pos + seq_len), freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)\n",
    "    \n",
    "    # Apply rotation\n",
    "    x_split = x.float().view(*x.shape[:-1], -1, 2)\n",
    "    x_complex = torch.view_as_complex(x_split)\n",
    "    x_rotated = x_complex * freqs_cis\n",
    "    return torch.view_as_real(x_rotated).flatten(-2).type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 1024\n",
    "EMBD_DIM = 512\n",
    "kv_cache = [{\n",
    "    'k': torch.zeros([SEQ_LEN, EMBD_DIM]),\n",
    "    'v': torch.zeros([SEQ_LEN, EMBD_DIM])\n",
    "} for i in range(config['n_layers'])]\n",
    "\n",
    "tokens = tokens_i.clone()\n",
    "x = embd(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_prefill_stage = True\n",
    "kv_counter = 0\n",
    "while tokens[-1] < 128000:\n",
    "    with torch.no_grad():\n",
    "        x = embd(tokens)\n",
    "\n",
    "        if not is_prefill_stage:\n",
    "            x = x[-1:, :]\n",
    "    \n",
    "        seq_len = x.shape[0]\n",
    "        head_dim = config['dim'] // config['n_heads']\n",
    "\n",
    "        for layer in range(config['n_layers']):\n",
    "            # RMS 1\n",
    "            rms_1 = torch.nn.functional.rms_norm(x, normalized_shape=(x.shape[-1],), weight=model[f\"layers.{layer}.attention_norm.weight\"], eps=config[\"norm_eps\"])\n",
    "            \n",
    "            # GQA\n",
    "            xq = rms_1 @ torch.transpose(model[f\"layers.{layer}.attention.wq.weight\"].type(torch.float32), 0, 1)\n",
    "            xk = rms_1 @ torch.transpose(model[f\"layers.{layer}.attention.wk.weight\"].type(torch.float32), 0, 1)\n",
    "            xv = rms_1 @ torch.transpose(model[f\"layers.{layer}.attention.wv.weight\"].type(torch.float32), 0, 1)\n",
    "\n",
    "            kv_cache[layer]['k'][kv_counter:kv_counter+seq_len] = xk\n",
    "            kv_cache[layer]['v'][kv_counter:kv_counter+seq_len] = xv\n",
    "\n",
    "            if not is_prefill_stage:\n",
    "                xk = torch.concat((kv_cache[layer]['k'][:kv_counter], xk), dim=0)\n",
    "                xv = torch.concat((kv_cache[layer]['v'][:kv_counter], xv), dim=0)\n",
    "\n",
    "            xq = xq.view(seq_len, config['n_heads'], head_dim).transpose(0, 1).contiguous()\n",
    "            xk = xk.view(kv_counter+seq_len, config['n_kv_heads'], head_dim) \\\n",
    "                .unsqueeze(2) \\\n",
    "                .expand(-1, -1, 4, -1) \\\n",
    "                .flatten(1, 2) \\\n",
    "                .transpose(0, 1) \\\n",
    "                .contiguous()\n",
    "            xv = xv.view(kv_counter+seq_len, config['n_kv_heads'], head_dim) \\\n",
    "                .unsqueeze(2) \\\n",
    "                .expand(-1, -1, 4, -1) \\\n",
    "                .flatten(1, 2) \\\n",
    "                .transpose(0, 1) \\\n",
    "                .contiguous()\n",
    "\n",
    "            xq = apply_rope(xq, start_pos=kv_counter)\n",
    "            xk = apply_rope(xk)\n",
    "            \n",
    "            logits = (xq @ xk.transpose(-2, -1)) / (head_dim**0.5)\n",
    "\n",
    "            mask = torch.triu(\n",
    "                torch.full((kv_counter+seq_len, kv_counter+seq_len), float('-inf')),\n",
    "                diagonal=1\n",
    "            )\n",
    "            attn_i = (logits + mask[-seq_len:, :]).softmax(dim=-1)\n",
    "            if not is_prefill_stage:\n",
    "                attn_i = attn_i[:, -1:, :]\n",
    "            attn = attn_i @ xv\n",
    "            attn_o = attn.transpose(0, 1).reshape(seq_len, -1) @ model[f\"layers.{layer}.attention.wo.weight\"].type(torch.float32).transpose(0, 1)\n",
    "            \n",
    "            # Residuals\n",
    "            x_2 = x + attn_o\n",
    "            \n",
    "            # RMS 2\n",
    "            rms_2 = torch.nn.functional.rms_norm(x_2, normalized_shape=(x_2.shape[-1],), weight=model[f\"layers.{layer}.ffn_norm.weight\"], eps=config[\"norm_eps\"])\n",
    "            \n",
    "            # FFN\n",
    "            ffn_w1 = model[f\"layers.{layer}.feed_forward.w1.weight\"].type(torch.float32)\n",
    "            ffn_w2 = model[f\"layers.{layer}.feed_forward.w2.weight\"].type(torch.float32)\n",
    "            ffn_w3 = model[f\"layers.{layer}.feed_forward.w3.weight\"].type(torch.float32)\n",
    "            ffn_output = (torch.nn.functional.silu(rms_2 @ torch.transpose(ffn_w1, 0, 1)) * (rms_2 @ torch.transpose(ffn_w3, 0, 1))) @ torch.transpose(ffn_w2, 0, 1)\n",
    "            \n",
    "            # Residuals\n",
    "            x = x_2 + ffn_output\n",
    "\n",
    "        kv_counter += seq_len\n",
    "\n",
    "        rms_f = torch.nn.functional.rms_norm(x, normalized_shape=(x.shape[-1],), weight=model[\"norm.weight\"], eps=config[\"norm_eps\"])\n",
    "        linear_f = rms_f @ torch.transpose(model[\"output.weight\"].type(torch.float32), 0, 1)\n",
    "        out = torch.nn.functional.softmax(linear_f, dim=-1)[-1]\n",
    "        values, indices = torch.topk(out, k=1)\n",
    "        next_token = indices[0].item()\n",
    "        print(tokenizer.decode([next_token]))\n",
    "        tokens = torch.cat((tokens, torch.tensor([next_token])))\n",
    "        is_prefill_stage = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3-py (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
