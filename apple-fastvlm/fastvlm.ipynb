{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761c239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omkaarwork/Desktop/projects/models-from-scratch/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "from safetensors.torch import load_file \n",
    "\n",
    "device = torch.device('mps')\n",
    "model = load_file(\"./model/model.safetensors\", device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8296113",
   "metadata": {},
   "source": [
    "### Vision Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551b0cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/0gyj64797gz462blnnsqxz9w0000gn/T/ipykernel_29508/234447640.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  img_tensor = torch.tensor(CLIPImageProcessor(\n"
     ]
    }
   ],
   "source": [
    "# PREPARE INPUT\n",
    "image = Image.open('./lion.jpg').convert(\"RGB\")\n",
    "\n",
    "img_tensor = torch.tensor(CLIPImageProcessor(\n",
    "    crop_size={\"height\": 1024, \"width\": 1024},\n",
    "    image_mean=[0.0, 0.0, 0.0],\n",
    "    image_std=[1.0, 1.0, 1.0],\n",
    "    size={\"shortest_edge\": 1024},\n",
    "    return_tensors='pt'\n",
    ")(image)['pixel_values'], device=torch.device('mps'), dtype=torch.bfloat16) \n",
    "\n",
    "# [1, 3, 1024, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c61ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEM\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        img_tensor,\n",
    "        model[f'model.vision_tower.vision_tower.model.patch_embed.0.reparam_conv.weight'], \n",
    "        bias=model[f'model.vision_tower.vision_tower.model.patch_embed.0.reparam_conv.bias'],\n",
    "        stride=2,\n",
    "        padding=1,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    ").to(device, dtype=torch.bfloat16)\n",
    "\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "    x,\n",
    "    model[f'model.vision_tower.vision_tower.model.patch_embed.1.reparam_conv.weight'], \n",
    "    bias=model[f'model.vision_tower.vision_tower.model.patch_embed.1.reparam_conv.bias'], \n",
    "    stride=2,\n",
    "    padding=1,\n",
    "    groups=96,\n",
    "    dilation=1\n",
    ")).to(device, dtype=torch.bfloat16)\n",
    "\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        model[f'model.vision_tower.vision_tower.model.patch_embed.2.reparam_conv.weight'], \n",
    "        bias=model['model.vision_tower.vision_tower.model.patch_embed.2.reparam_conv.bias'], \n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# [1, 96, 256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2d3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "for i in range(2):\n",
    "    # TOKENMIXER - RepMixer\n",
    "    x = torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.0.{i}.token_mixer.reparam_conv.weight'], \n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.0.{i}.token_mixer.reparam_conv.bias'],\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        groups=96,\n",
    "        dilation=1\n",
    "    )\n",
    "\n",
    "    # CONVFFN\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.conv.conv.weight'],\n",
    "        stride=1,\n",
    "        padding=3,\n",
    "        groups=96,\n",
    "        dilation=1\n",
    "    )\n",
    "    x_c = torch.nn.functional.batch_norm(\n",
    "        x_c,\n",
    "        running_mean=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.conv.bn.running_mean'],\n",
    "        running_var=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.conv.bn.running_var'],\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.conv.bn.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.conv.bn.bias'],\n",
    "        training=False,\n",
    "    )\n",
    "    x_c = torch.nn.functional.gelu(\n",
    "        torch.nn.functional.conv2d(\n",
    "            x_c,\n",
    "            weight=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.fc1.weight'],\n",
    "            bias=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.fc1.bias'],\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            dilation=1\n",
    "        )\n",
    "    )\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x_c,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.fc2.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.0.{i}.convffn.fc2.bias'],\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    "    x = x + model[f'model.vision_tower.vision_tower.model.network.0.{i}.layer_scale'].view(1, -1, 1, 1) * x_c\n",
    "\n",
    "# [1, 96, 256, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e6d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH EMBED\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model['model.vision_tower.vision_tower.model.network.1.proj.0.lkb_reparam.weight'],\n",
    "        bias=model['model.vision_tower.vision_tower.model.network.1.proj.0.lkb_reparam.bias'],\n",
    "        stride=2,\n",
    "        padding=3,\n",
    "        groups=96,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model['model.vision_tower.vision_tower.model.network.1.proj.1.reparam_conv.weight'],\n",
    "        bias=model['model.vision_tower.vision_tower.model.network.1.proj.1.reparam_conv.bias'],\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# [1, 192, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9e549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 2\n",
    "layer_idx = 2\n",
    "for i in range(12):\n",
    "    # TOKENMIXER - RepMixer\n",
    "    x = torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.2.{i}.token_mixer.reparam_conv.weight'], \n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.2.{i}.token_mixer.reparam_conv.bias'],\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        groups=192,\n",
    "        dilation=1\n",
    "    )\n",
    "\n",
    "    # CONVFFN\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.conv.conv.weight'],\n",
    "        stride=1,\n",
    "        padding=3,\n",
    "        groups=192,\n",
    "        dilation=1\n",
    "    )\n",
    "    x_c = torch.nn.functional.batch_norm(\n",
    "        x_c,\n",
    "        running_mean=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.conv.bn.running_mean'],\n",
    "        running_var=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.conv.bn.running_var'],\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.conv.bn.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.conv.bn.bias'],\n",
    "        training=False,\n",
    "    )\n",
    "    x_c = torch.nn.functional.gelu(\n",
    "        torch.nn.functional.conv2d(\n",
    "            x_c,\n",
    "            weight=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.fc1.weight'],\n",
    "            bias=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.fc1.bias'],\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            dilation=1\n",
    "        )\n",
    "    )\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x_c,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.fc2.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.2.{i}.convffn.fc2.bias'],\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    "    x = x + model[f'model.vision_tower.vision_tower.model.network.2.{i}.layer_scale'].view(1, -1, 1, 1) * x_c\n",
    "\n",
    "# [1, 192, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "818f5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH EMBED\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model['model.vision_tower.vision_tower.model.network.3.proj.0.lkb_reparam.weight'],\n",
    "        bias=model['model.vision_tower.vision_tower.model.network.3.proj.0.lkb_reparam.bias'],\n",
    "        stride=2,\n",
    "        padding=3,\n",
    "        groups=192,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model['model.vision_tower.vision_tower.model.network.3.proj.1.reparam_conv.weight'],\n",
    "        bias=model['model.vision_tower.vision_tower.model.network.3.proj.1.reparam_conv.bias'],\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# [1, 384, 64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0162b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 3\n",
    "layer_idx = 4\n",
    "for i in range(24):\n",
    "    # TOKENMIXER - RepMixer\n",
    "    x = torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.4.{i}.token_mixer.reparam_conv.weight'], \n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.4.{i}.token_mixer.reparam_conv.bias'],\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        groups=384,\n",
    "        dilation=1\n",
    "    )\n",
    "\n",
    "    # CONVFFN\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.conv.conv.weight'],\n",
    "        stride=1,\n",
    "        padding=3,\n",
    "        groups=384,\n",
    "        dilation=1\n",
    "    )\n",
    "    x_c = torch.nn.functional.batch_norm(\n",
    "        x_c,\n",
    "        running_mean=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.conv.bn.running_mean'],\n",
    "        running_var=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.conv.bn.running_var'],\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.conv.bn.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.conv.bn.bias'],\n",
    "        training=False,\n",
    "    )\n",
    "    x_c = torch.nn.functional.gelu(\n",
    "        torch.nn.functional.conv2d(\n",
    "            x_c,\n",
    "            weight=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.fc1.weight'],\n",
    "            bias=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.fc1.bias'],\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            dilation=1\n",
    "        )\n",
    "    )\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x_c,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.fc2.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.4.{i}.convffn.fc2.bias'],\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    "    x = x + model[f'model.vision_tower.vision_tower.model.network.4.{i}.layer_scale'].view(1, -1, 1, 1) * x_c\n",
    "\n",
    "# [1, 384, 64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb7320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH EMBED\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model['model.vision_tower.vision_tower.model.network.5.proj.0.lkb_reparam.weight'],\n",
    "        bias=model['model.vision_tower.vision_tower.model.network.5.proj.0.lkb_reparam.bias'],\n",
    "        stride=2,\n",
    "        padding=3,\n",
    "        groups=384,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model['model.vision_tower.vision_tower.model.network.5.proj.1.reparam_conv.weight'],\n",
    "        bias=model['model.vision_tower.vision_tower.model.network.5.proj.1.reparam_conv.bias'],\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# [1, 768, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b66e5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Positional Embeddings (CPE)\n",
    "x = torch.nn.functional.conv2d(\n",
    "    x,\n",
    "    weight=model[f'model.vision_tower.vision_tower.model.network.6.reparam_conv.weight'],\n",
    "    bias=model[f'model.vision_tower.vision_tower.model.network.6.reparam_conv.bias'],\n",
    "    stride=1,\n",
    "    padding=3,\n",
    "    groups=768,\n",
    "    dilation=1\n",
    ")\n",
    "\n",
    "# [1, 768, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2ca7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 4\n",
    "layer_idx = 7\n",
    "for i in range(4):    \n",
    "    # TOKENMIXER - Attention\n",
    "\n",
    "    # Since format is (B, C, H, W) and I don't want to reshape to put C in the end, we roll our own layernorm2d\n",
    "    mean = x.mean(dim=1, keepdim=True)\n",
    "    var = x.var(dim=1, unbiased=False, keepdim=True)\n",
    "    x_hat = (x - mean) / torch.sqrt(var + 1e-5)\n",
    "    x_norm = model[f'model.vision_tower.vision_tower.model.network.7.{i}.norm.weight'][:, None, None] \\\n",
    "        * x_hat \\\n",
    "        + model[f'model.vision_tower.vision_tower.model.network.7.{i}.norm.bias'][:, None, None] # [:, None, None] makes (768) -> (768, 1, 1) making multiply with (1, 768, 32, 32) valid\n",
    "\n",
    "    head_dim = 32\n",
    "    B, C, H, W = x_norm.shape[:]\n",
    "    n_heads = C // head_dim\n",
    "    N = H * W # pixels\n",
    "    x_norm = x_norm.flatten(2).transpose(-2, -1)\n",
    "    qkv: torch.Tensor = (\n",
    "        (x_norm @ model[f'model.vision_tower.vision_tower.model.network.7.{i}.token_mixer.qkv.weight'].T)\n",
    "        .reshape(B, N, 3, n_heads, head_dim) # B, N, C, H, D\n",
    "        .permute(2, 0, 3, 1, 4) # C, B, H, N, D = 3, 1, 24, 1024, 32\n",
    "    )\n",
    "    q, k, v = qkv.unbind(0) # (B, 24, 1024, 32)\n",
    "    score = (q @ k.transpose(-1,-2)) / (head_dim ** 0.5) # (B, 24, 1024, 1024)\n",
    "    score = score.softmax(dim=-1)\n",
    "    out = score @ v # (B, 24, 1024, 32)\n",
    "    out = out.transpose(1, 2).reshape(B, N, C)\n",
    "    x_attn = torch.nn.functional.linear(out, model[f'model.vision_tower.vision_tower.model.network.7.{i}.token_mixer.proj.weight'], model[f'model.vision_tower.vision_tower.model.network.7.{i}.token_mixer.proj.bias'])\n",
    "    x_attn = x_attn.transpose(1, 2).reshape(B, C, H, W)\n",
    "    x = x + model[f'model.vision_tower.vision_tower.model.network.7.{i}.layer_scale_1'].view(1, -1, 1, 1) * x_attn\n",
    "\n",
    "    # CONVFFN\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.conv.conv.weight'],\n",
    "        stride=1,\n",
    "        padding=3,\n",
    "        groups=768,\n",
    "        dilation=1\n",
    "    )\n",
    "    x_c = torch.nn.functional.batch_norm(\n",
    "        x_c,\n",
    "        running_mean=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.conv.bn.running_mean'],\n",
    "        running_var=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.conv.bn.running_var'],\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.conv.bn.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.conv.bn.bias'],\n",
    "        training=False,\n",
    "    )\n",
    "    x_c = torch.nn.functional.gelu(\n",
    "        torch.nn.functional.conv2d(\n",
    "            x_c,\n",
    "            weight=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.fc1.weight'],\n",
    "            bias=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.fc1.bias'],\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            dilation=1\n",
    "        )\n",
    "    )\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x_c,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.fc2.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.7.{i}.convffn.fc2.bias'],\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    "    x = x + model[f'model.vision_tower.vision_tower.model.network.7.{i}.layer_scale_2'].view(1, -1, 1, 1) * x_c\n",
    "\n",
    "# [1, 768, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e3c4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH EMBED\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x,  # (B, 768, 32, 32)\n",
    "        weight=model['model.vision_tower.vision_tower.model.network.8.proj.0.lkb_reparam.weight'],\n",
    "        bias=model['model.vision_tower.vision_tower.model.network.8.proj.0.lkb_reparam.bias'],\n",
    "        stride=2,\n",
    "        padding=3,\n",
    "        groups=768,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "x = torch.nn.functional.gelu(\n",
    "    torch.nn.functional.conv2d(\n",
    "        x, # (B, 768, 16, 16)\n",
    "        weight=model['model.vision_tower.vision_tower.model.network.8.proj.1.reparam_conv.weight'],\n",
    "        bias=model['model.vision_tower.vision_tower.model.network.8.proj.1.reparam_conv.bias'],\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        groups=1,\n",
    "        dilation=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# [1, 1536, 16, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6c6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPCPE\n",
    "x = torch.nn.functional.conv2d(\n",
    "    x,\n",
    "    weight=model['model.vision_tower.vision_tower.model.network.9.reparam_conv.weight'],\n",
    "    bias=model['model.vision_tower.vision_tower.model.network.9.reparam_conv.bias'],\n",
    "    stride=1,\n",
    "    padding=3,\n",
    "    groups=1536,\n",
    "    dilation=1\n",
    ")\n",
    "\n",
    "# [1, 1536, 16, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e37145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 5\n",
    "for i in range(2):\n",
    "    # TOKENMIXER - Attention\n",
    "\n",
    "    mean = x.mean(dim=1, keepdim=True)\n",
    "    var = x.var(dim=1, unbiased=False, keepdim=True)\n",
    "    x_hat = (x - mean) / torch.sqrt(var + 1e-5)\n",
    "    x_norm = (\n",
    "        model[f'model.vision_tower.vision_tower.model.network.10.{i}.norm.weight'][:, None, None] * x_hat\n",
    "        + model[f'model.vision_tower.vision_tower.model.network.10.{i}.norm.bias'][:, None, None]\n",
    "    )\n",
    "\n",
    "    head_dim = 32\n",
    "    B, C, H, W = x_norm.shape\n",
    "    n_heads = C // head_dim\n",
    "    N = H * W\n",
    "\n",
    "    x_tokens = x_norm.flatten(2).transpose(-2, -1) # [B, N, C]\n",
    "    qkv = (\n",
    "        (x_tokens @ model[f'model.vision_tower.vision_tower.model.network.10.{i}.token_mixer.qkv.weight'].T)\n",
    "        .reshape(B, N, 3, n_heads, head_dim)\n",
    "        .permute(2, 0, 3, 1, 4)\n",
    "    )\n",
    "    q, k, v = qkv.unbind(0) # [B, n_heads, N, head_dim]\n",
    "\n",
    "    scores = (q @ k.transpose(-1, -2)) / (head_dim ** 0.5) # [B, n_heads, N, N]\n",
    "    attn = scores.softmax(dim=-1)\n",
    "    out = attn @ v # [B, n_heads, N, head_dim]\n",
    "\n",
    "    out = out.transpose(1, 2).reshape(B, N, C) # [B, N, C]\n",
    "    x_attn = torch.nn.functional.linear(\n",
    "        out,\n",
    "        model[f'model.vision_tower.vision_tower.model.network.10.{i}.token_mixer.proj.weight'],\n",
    "        model[f'model.vision_tower.vision_tower.model.network.10.{i}.token_mixer.proj.bias'],\n",
    "    )\n",
    "    x_attn = x_attn.transpose(1, 2).reshape(B, C, H, W)\n",
    "\n",
    "    x = x + model[f'model.vision_tower.vision_tower.model.network.10.{i}.layer_scale_1'].view(1, -1, 1, 1) * x_attn\n",
    "\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.conv.conv.weight'],\n",
    "        padding=3,\n",
    "        groups=1536,\n",
    "    )\n",
    "    x_c = torch.nn.functional.batch_norm(\n",
    "        x_c,\n",
    "        running_mean=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.conv.bn.running_mean'],\n",
    "        running_var=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.conv.bn.running_var'],\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.conv.bn.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.conv.bn.bias'],\n",
    "        training=False,\n",
    "    )\n",
    "    x_c = torch.nn.functional.gelu(\n",
    "        torch.nn.functional.conv2d(\n",
    "            x_c,\n",
    "            weight=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.fc1.weight'],\n",
    "            bias=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.fc1.bias'],\n",
    "            padding=0,\n",
    "        )\n",
    "    )\n",
    "    x_c = torch.nn.functional.conv2d(\n",
    "        x_c,\n",
    "        weight=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.fc2.weight'],\n",
    "        bias=model[f'model.vision_tower.vision_tower.model.network.10.{i}.convffn.fc2.bias'],\n",
    "        padding=0,\n",
    "    )\n",
    "    x = x + model[f'model.vision_tower.vision_tower.model.network.10.{i}.layer_scale_2'].view(1, -1, 1, 1) * x_c\n",
    "\n",
    "# [1, 1536, 16, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c150b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOBILE BLOCK\n",
    "x = torch.nn.functional.conv2d(\n",
    "    x,\n",
    "    weight=model['model.vision_tower.vision_tower.model.conv_exp.reparam_conv.weight'],\n",
    "    bias=model['model.vision_tower.vision_tower.model.conv_exp.reparam_conv.bias'],\n",
    "    stride=1,\n",
    "    padding=1,\n",
    "    dilation=1,\n",
    "    groups=1536,\n",
    ")\n",
    "\n",
    "# SQUEEZE-AND-EXCITE MODULE\n",
    "B, C, H, W = x.shape\n",
    "x_se = torch.nn.functional.avg_pool2d(x, kernel_size=[16, 16]) # (B, C, 1, 1)\n",
    "x_se = torch.nn.functional.conv2d(\n",
    "    x_se,\n",
    "    weight=model['model.vision_tower.vision_tower.model.conv_exp.se.reduce.weight'],\n",
    "    bias=model['model.vision_tower.vision_tower.model.conv_exp.se.reduce.bias'],\n",
    "    stride=1\n",
    ")\n",
    "x_se = torch.nn.functional.relu(x_se)\n",
    "x_se = torch.nn.functional.conv2d(\n",
    "    x_se,\n",
    "    weight=model['model.vision_tower.vision_tower.model.conv_exp.se.expand.weight'],\n",
    "    bias=model['model.vision_tower.vision_tower.model.conv_exp.se.expand.bias'],\n",
    "    stride=1\n",
    ")\n",
    "x_se = torch.sigmoid(x_se)\n",
    "x = torch.nn.functional.gelu(x * x_se)\n",
    "\n",
    "# [1, 3072, 16, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f22437bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECTION\n",
    "img_tokens = x.flatten(2).transpose(1, 2)\n",
    "h = torch.nn.functional.gelu(torch.nn.functional.linear(img_tokens, model['model.mm_projector.0.weight'], model['model.mm_projector.0.bias']))\n",
    "img_tokens = torch.nn.functional.linear(h, model['model.mm_projector.2.weight'], model['model.mm_projector.2.bias'])\n",
    "\n",
    "# [1, 256, 1536] = 256 tokens of the llm's embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9d54e",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6c0796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and embed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model')\n",
    "with open(\"./model/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "embd = torch.nn.Embedding(config[\"vocab_size\"], config[\"hidden_size\"], device='mps', dtype=torch.bfloat16)\n",
    "embd.load_state_dict({\"weight\": model[\"model.embed_tokens.weight\"]})\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"<image>\\nDescribe this image in detail.\"}]\n",
    "\n",
    "rendered = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "pre, post = rendered.split(\"<image>\", 1)\n",
    "pre_ids = tokenizer(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids.to('mps')\n",
    "post_ids = tokenizer(post, return_tensors=\"pt\", add_special_tokens=False).input_ids.to('mps')\n",
    "emb_pre = embd(pre_ids)\n",
    "emb_post = embd(post_ids)\n",
    "\n",
    "x = torch.cat([emb_pre, img_tokens, emb_post], dim=1) # [1, S, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d56b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE\n",
    "\n",
    "def rotate_half(t: torch.Tensor) -> torch.Tensor:\n",
    "    d2 = t.shape[-1] // 2\n",
    "    return torch.cat((-t[..., d2:], t[..., :d2]), dim=-1)\n",
    "\n",
    "def apply_rope(qk: torch.Tensor, base: float) -> torch.Tensor:\n",
    "    _, _, S, D = qk.shape\n",
    "    assert D % 2 == 0\n",
    "    inv = torch.arange(0, D, 2, device=qk.device, dtype=torch.float32) / D\n",
    "    inv_freq = base ** (-inv) # (D/2,)\n",
    "    t = torch.arange(S, device=qk.device, dtype=torch.float32) # (S,)\n",
    "    freqs = torch.einsum(\"s,d->sd\", t, inv_freq) # (S, D/2)\n",
    "    emb = torch.cat([freqs, freqs], dim=-1) # (S, D)\n",
    "    cos = emb.cos().to(qk.dtype)[None, None, :, :] # (1,1,S,D)\n",
    "    sin = emb.sin().to(qk.dtype)[None, None, :, :]\n",
    "    return (qk * cos) + (rotate_half(qk) * sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc06f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a detailed bronze statue of a lion, positioned on a stone pedestal. The lion is lying down with its front paws extended forward and its head turned to the right, showcasing its open mouth and visible teeth. The statue is highly realistic, capturing the lion's muscular build and intricate details, including its mane and facial features. The lion is situated outdoors, with a backdrop of a clear blue sky adorned with fluffy white clouds. In the distance, there is a prominent building featuring a large dome and a clock tower, which appears to be a church or a significant historical structure. The building is surrounded by lush green trees, adding to the serene and majestic atmosphere of the scene. The overall composition of the image highlights the grandeur and artistic craftsmanship of the lion statue against the picturesque and tranquil setting."
     ]
    }
   ],
   "source": [
    "generated_id = None\n",
    "with torch.no_grad():\n",
    "    while generated_id == None or generated_id != tokenizer.eos_token_id:\n",
    "        S = x.shape[1]\n",
    "        hidden = config[\"hidden_size\"]\n",
    "        n_heads = config[\"num_attention_heads\"]\n",
    "        n_kv = config[\"num_key_value_heads\"]\n",
    "        head_dim = hidden // n_heads\n",
    "        h = x\n",
    "        for layer in range(config[\"num_hidden_layers\"]):\n",
    "            # RMSNorm\n",
    "            h_rms = torch.nn.functional.rms_norm(\n",
    "                h, normalized_shape=(hidden,),\n",
    "                weight=model[f\"model.layers.{layer}.input_layernorm.weight\"],\n",
    "                eps=config[\"rms_norm_eps\"],\n",
    "            ).to(torch.bfloat16)\n",
    "\n",
    "            # QKV\n",
    "            q = h_rms @ model[f\"model.layers.{layer}.self_attn.q_proj.weight\"].T + model[f\"model.layers.{layer}.self_attn.q_proj.bias\"]\n",
    "            k = h_rms @ model[f\"model.layers.{layer}.self_attn.k_proj.weight\"].T + model[f\"model.layers.{layer}.self_attn.k_proj.bias\"]\n",
    "            v = h_rms @ model[f\"model.layers.{layer}.self_attn.v_proj.weight\"].T + model[f\"model.layers.{layer}.self_attn.v_proj.bias\"]\n",
    "\n",
    "            q = q.view(1, S, n_heads, head_dim).transpose(1, 2) # [1,H,S,D]\n",
    "            k = k.view(1, S, n_kv,   head_dim).transpose(1, 2) # [1,KV,S,D]\n",
    "            v = v.view(1, S, n_kv,   head_dim).transpose(1, 2)\n",
    "\n",
    "            if n_heads != n_kv:\n",
    "                reps = n_heads // n_kv\n",
    "                k = k.repeat_interleave(reps, dim=1)\n",
    "                v = v.repeat_interleave(reps, dim=1)\n",
    "\n",
    "            # RoPE\n",
    "            theta = config[\"rope_theta\"]\n",
    "            q = apply_rope(q, theta)\n",
    "            k = apply_rope(k, theta)\n",
    "\n",
    "            # Attn\n",
    "            scores = (q @ k.transpose(-2, -1)) / (head_dim ** 0.5) # [1,H,S,S]\n",
    "            causal = torch.triu(torch.full((S, S), float(\"-inf\"), device='mps', dtype=scores.dtype), 1)\n",
    "            attn = torch.softmax(scores + causal, dim=-1)\n",
    "            attn_o = (attn @ v).transpose(1, 2).reshape(1, S, hidden) @ model[f\"model.layers.{layer}.self_attn.o_proj.weight\"].T\n",
    "\n",
    "            h = h + attn_o\n",
    "\n",
    "            # FFN\n",
    "            h_rms = torch.nn.functional.rms_norm(\n",
    "                h, normalized_shape=(hidden,),\n",
    "                weight=model[f\"model.layers.{layer}.post_attention_layernorm.weight\"],\n",
    "                eps=config[\"rms_norm_eps\"],\n",
    "            ).to(torch.bfloat16)\n",
    "\n",
    "            gate = h_rms @ model[f\"model.layers.{layer}.mlp.gate_proj.weight\"].T\n",
    "            up = h_rms @ model[f\"model.layers.{layer}.mlp.up_proj.weight\"].T\n",
    "            ffn = (torch.nn.functional.silu(gate) * up) @ model[f\"model.layers.{layer}.mlp.down_proj.weight\"].T\n",
    "\n",
    "            h = h + ffn\n",
    "\n",
    "        # Final norm + logits\n",
    "        h_norm = torch.nn.functional.rms_norm(\n",
    "            h, normalized_shape=(hidden,),\n",
    "            weight=model[\"model.norm.weight\"],\n",
    "            eps=config[\"rms_norm_eps\"],\n",
    "        ).to(torch.bfloat16)\n",
    "\n",
    "        logits = h_norm @ model[\"lm_head.weight\"].T\n",
    "        generated_id = int(logits[:, -1, :].argmax(dim=-1))\n",
    "        if generated_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        next_embed = embd(torch.tensor([[generated_id]], device='mps'))\n",
    "        x = torch.cat([x, next_embed], dim=1)\n",
    "        \n",
    "        print(tokenizer.decode(generated_id, skip_special_tokens=True), end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3-py (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
