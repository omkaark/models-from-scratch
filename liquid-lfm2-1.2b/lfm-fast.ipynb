{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a small bug in this caching impl compared to non-caching and I can't figure out where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b0ba83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run `HF_HUB_ENABLE_HF_TRANSFER=1 uv run hf download LiquidAI/LFM2-1.2B --local-dir /Users/omkaarwork/Desktop/projects/models-from-scratch/liquid-lsm2-1.2b/model`\n",
    "tokenizer_path = \"./model\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20313f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model/config.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14133610",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_file(\"./model/model.safetensors\", device=device.type)\n",
    "embd = torch.nn.Embedding(config['vocab_size'], config['hidden_size'])\n",
    "embd.load_state_dict({'weight': model[f'model.embed_tokens.weight']})\n",
    "embd = embd.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "948f73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftext|>', '<|im_start|>', 'system', '\\n', 'Follow', ' the', ' instructions', '.', '<|im_end|>', '\\n', '<|im_start|>', 'user', '\\n', 'Re', 'peat', ' this', ' sentence', \" '\", 'hi', ' this', ' is', ' l', 'fm', \"'\", ' and', ' end', ' your', ' turn', '<|im_end|>', '\\n', '<|im_start|>', 'assistant', '\\n']\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": 'Follow the instructions.'},\n",
    "    {\"role\": \"user\",   \"content\": \"Repeat this sentence 'hi this is lfm' and end your turn\"}\n",
    "]\n",
    "tokens_i = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=True, add_generation_prompt=True\n",
    ")\n",
    "tokens_i = torch.tensor(tokens_i, device=device)\n",
    "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens_i]\n",
    "print(prompt_split_as_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8f01471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x: torch.Tensor, start: int = 0):\n",
    "    _, H, S, D =  x.shape\n",
    "    freqs = 1 / config['rope_theta'] ** (torch.arange(0, D, 2, device=x.device, dtype=torch.float32) / D)\n",
    "    positions = torch.arange(start, start + S, device=x.device, dtype=torch.float32)\n",
    "    freqs_per_token = torch.outer(positions, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs_per_token), freqs_per_token)\n",
    "    x_pairs = x.view(*x.shape[:-1], -1, 2)\n",
    "    x_complex = torch.view_as_complex(x_pairs)\n",
    "    x_rotated = x_complex * freqs_cis\n",
    "    return torch.view_as_real(x_rotated).flatten(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04687dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens_i.clone()\n",
    "kv_cache = {l: {'k': None, 'v': None} for l in config['full_attn_idxs']}\n",
    "conv_cache = {l: None for l in range(config['num_hidden_layers']) if l not in config['full_attn_idxs']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56ba4175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, this is lfm.\n",
      "\n",
      "(Note: The last part \"Hi, this is lfm"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefill_done \u001b[38;5;129;01mand\u001b[39;00m S == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m conv_cache[layer] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     combined = torch.cat((conv_cache[layer], pre_conv), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     y = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel.layers.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.conv.conv.weight\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhidden_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     x_c = y[:, :, -\u001b[32m1\u001b[39m:]\n\u001b[32m     65\u001b[39m     keep = config[\u001b[33m'\u001b[39m\u001b[33mconv_L_cache\u001b[39m\u001b[33m'\u001b[39m] - \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "prefill_done = False\n",
    "\n",
    "with torch.inference_mode():\n",
    "    while tokens[-1].item() != tokenizer.eos_token_id:\n",
    "\n",
    "        x = embd(tokens).unsqueeze(0) if not prefill_done else embd(tokens[-1:]).unsqueeze(0)\n",
    "\n",
    "        for layer in range(config['num_hidden_layers']):\n",
    "            S = x.shape[1]\n",
    "\n",
    "            x_norm = torch.nn.functional.rms_norm(x, normalized_shape=(config['hidden_size'],), weight=model[f'model.layers.{layer}.operator_norm.weight'], eps=config['norm_eps']).type(torch.bfloat16)\n",
    "\n",
    "            if layer in config['full_attn_idxs']:  # Attention\n",
    "                xq = x_norm @ model[f'model.layers.{layer}.self_attn.q_proj.weight'].type(torch.bfloat16).T\n",
    "                xk = x_norm @ model[f'model.layers.{layer}.self_attn.k_proj.weight'].type(torch.bfloat16).T\n",
    "                xv = x_norm @ model[f'model.layers.{layer}.self_attn.v_proj.weight'].type(torch.bfloat16).T\n",
    "\n",
    "                xq = xq.view(1, S, config['num_attention_heads'], -1).transpose(-2, -3)\n",
    "                xk = xk.view(1, S, config['num_key_value_heads'], -1).repeat_interleave(config['num_attention_heads'] // config['num_key_value_heads'], dim=2).transpose(-2, -3)\n",
    "                xv = xv.view(1, S, config['num_key_value_heads'], -1).repeat_interleave(config['num_attention_heads'] // config['num_key_value_heads'], dim=2).transpose(-2, -3)\n",
    "\n",
    "                start_idx = kv_cache[layer]['k'].shape[-2] if (prefill_done and kv_cache[layer]['k'] is not None) else 0\n",
    "                xq = apply_rope(xq.type(torch.float32), start_idx)\n",
    "                xk = apply_rope(xk.type(torch.float32), start_idx)\n",
    "\n",
    "                xq = torch.nn.functional.rms_norm(xq, normalized_shape=(xq.shape[-1],), weight=model[f'model.layers.{layer}.self_attn.q_layernorm.weight'], eps=config['norm_eps'])\n",
    "                xk = torch.nn.functional.rms_norm(xk, normalized_shape=(xk.shape[-1],), weight=model[f'model.layers.{layer}.self_attn.k_layernorm.weight'], eps=config['norm_eps'])\n",
    "                \n",
    "                if prefill_done and kv_cache[layer]['k'] is not None and S == 1:\n",
    "                    Kprev = kv_cache[layer]['k']\n",
    "                    Vprev = kv_cache[layer]['v']\n",
    "                    q_last = xq[:, :, 0:, :]\n",
    "                    score = (q_last @ Kprev.transpose(-1, -2)) / (Kprev.shape[-1] ** 0.5)\n",
    "                    attn = torch.softmax(score, dim=-1) @ Vprev\n",
    "                    x_operator = attn.to(torch.bfloat16).transpose(1, 2).reshape(1, 1, -1) @ model[f'model.layers.{layer}.self_attn.out_proj.weight'].type(torch.bfloat16).T\n",
    "                    kv_cache[layer]['k'] = torch.cat((Kprev, xk), dim=-2)\n",
    "                    kv_cache[layer]['v'] = torch.cat((Vprev, xv.float()), dim=-2)\n",
    "                else:\n",
    "                    score = ((xq @ xk.transpose(-1, -2)) / (xk.shape[-1] ** 0.5)).type(torch.float32) + torch.triu(torch.full((S, S), float('-inf'), device=x.device), diagonal=1)\n",
    "                    attn = torch.softmax(score, dim=-1) @ xv.float()\n",
    "                    x_operator = attn.to(torch.bfloat16).transpose(1, 2).reshape(1, S, -1) @ model[f'model.layers.{layer}.self_attn.out_proj.weight'].type(torch.bfloat16).T\n",
    "                    kv_cache[layer]['k'] = xk\n",
    "                    kv_cache[layer]['v'] = xv.float()\n",
    "            else: # Conv layer\n",
    "                # (1, S, D) @ (1, D, 3D) = (1, S, 3D) -> T -> (1, 3D, S)\n",
    "                BCx = (x_norm @ model[f'model.layers.{layer}.conv.in_proj.weight'].type(torch.bfloat16).T).transpose(-1, -2)\n",
    "\n",
    "                # (1, 3D, S) -> (1, D, S), (1, D, S), (1, D, S)\n",
    "                B, C, x_c = BCx.chunk(3, dim=-2)\n",
    "\n",
    "                # (1, D, S) * (1, D, S) -> (1, D, S)\n",
    "                x_c = B * x_c\n",
    "                pre_conv = x_c\n",
    "\n",
    "                # (1, D, S) conv (D, 1, 3) -> (1, D, S + 2)\n",
    "                if prefill_done and S == 1 and conv_cache[layer] is not None:\n",
    "                    combined = torch.cat((conv_cache[layer], pre_conv), dim=-1)\n",
    "                    y = torch.nn.functional.conv1d(\n",
    "                        combined,\n",
    "                        weight=model[f'model.layers.{layer}.conv.conv.weight'],\n",
    "                        padding=0,\n",
    "                        groups=config['hidden_size'],\n",
    "                    )\n",
    "                    x_c = y[:, :, -1:]\n",
    "                    keep = config['conv_L_cache'] - 1\n",
    "                    conv_cache[layer] = combined[:, :, -keep:] if keep > 0 else None\n",
    "                else:\n",
    "                    x_c = torch.nn.functional.conv1d(\n",
    "                        pre_conv,\n",
    "                        weight=model[f'model.layers.{layer}.conv.conv.weight'],\n",
    "                        padding=config['conv_L_cache'] - 1,\n",
    "                        groups=config['hidden_size'],\n",
    "                    )\n",
    "                    x_c = x_c[:, :, :S]\n",
    "                    keep = config['conv_L_cache'] - 1\n",
    "                    conv_cache[layer] = pre_conv[:, :, -keep:] if keep > 0 else None\n",
    "\n",
    "                # (1, D, S) * (1, D, S) -> (1, D, S)\n",
    "                x_c = C * x_c\n",
    "\n",
    "                # (1, S, D) @ (D, D) -> (1, S, D)\n",
    "                x_operator = x_c.transpose(-1, -2) @ model[f'model.layers.{layer}.conv.out_proj.weight'].type(torch.bfloat16).T\n",
    "\n",
    "            x = x + x_operator\n",
    "\n",
    "            x_norm = torch.nn.functional.rms_norm(x, normalized_shape=(config['hidden_size'],), weight=model[f'model.layers.{layer}.ffn_norm.weight'], eps=config['norm_eps']).type(torch.bfloat16)\n",
    "\n",
    "            ffn_w1 = model[f\"model.layers.{layer}.feed_forward.w1.weight\"].type(torch.bfloat16)\n",
    "            ffn_w2 = model[f\"model.layers.{layer}.feed_forward.w2.weight\"].type(torch.bfloat16)\n",
    "            ffn_w3 = model[f\"model.layers.{layer}.feed_forward.w3.weight\"].type(torch.bfloat16)\n",
    "            ffn_o = (torch.nn.functional.silu(x_norm @ torch.transpose(ffn_w1, 0, 1)) * (x_norm @ torch.transpose(ffn_w3, 0, 1))) @ torch.transpose(ffn_w2, 0, 1)\n",
    "\n",
    "            x = x + ffn_o\n",
    "\n",
    "        x = torch.nn.functional.rms_norm(x, normalized_shape=(config['hidden_size'],), weight=model['model.embedding_norm.weight'], eps=config['norm_eps']).type(torch.bfloat16)\n",
    "        out = x @ model[f'model.embed_tokens.weight'].T\n",
    "\n",
    "        next_id = out[:, -1, :].argmax(dim=-1)\n",
    "        tokens = torch.cat((tokens, next_id.to(device)), dim=-1)\n",
    "        print(tokenizer.decode([next_id.item()]), end='', flush=True)\n",
    "        prefill_done = True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3-py (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
